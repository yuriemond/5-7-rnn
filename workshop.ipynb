{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brave-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from os import listdir\n",
    "import nltk\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, TimeDistributed, Dense\n",
    "from tensorflow.keras.layers import Embedding, GRU\n",
    "import gensim\n",
    "import gensim.downloader as model_api\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-transparency",
   "metadata": {},
   "source": [
    "# 1. Sentiment analysis\n",
    "\n",
    "Using the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), we want to do a regression model that predict the ratings are on a 1-10 scale. You have an example train and test set in the `dataset` folder.\n",
    "\n",
    "### 1.1 Regression Model\n",
    "\n",
    "Use a feedforward neural network and NLP techniques we've seen up to now to train the best model you can on this dataset\n",
    "\n",
    "### 1.2 RNN model\n",
    "\n",
    "Train a RNN to do the sentiment analysis regression. The RNN should consist simply of an embedding layer (to make word IDs into word vectors) a recurrent blocks (GRU or LSTM) feeding into an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_files = listdir(\"data/train/pos\")\n",
    "neg_train_files = listdir(\"data/train/neg\")\n",
    "\n",
    "pos_test_files = listdir(\"data/test/pos\")\n",
    "neg_test_files = listdir(\"data/test/neg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(target, rev, files): \n",
    "    x = []\n",
    "    x_line = []\n",
    "\n",
    "    for file in files:\n",
    "        with open (f\"data/{target}/{rev}/{file}\", encoding=\"utf8\") as opened_file:\n",
    "            rating = file.split(\"_\")[1].split(\".\")[0]\n",
    "\n",
    "            for line in opened_file:\n",
    "                x_line = []\n",
    "                x_line.append(line)\n",
    "                x_line.append(rating)\n",
    "                x.append(x_line)\n",
    "                \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = pd.DataFrame(columns=[\"review\", \"rating\"], data=get_reviews(\"train\", \"pos\", pos_train_files))\n",
    "train_neg = pd.DataFrame(columns=[\"review\", \"rating\"], data=get_reviews(\"train\", \"neg\", neg_train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = pd.DataFrame(columns=[\"review\", \"rating\"], data=get_reviews(\"test\", \"pos\", pos_test_files))\n",
    "test_neg = pd.DataFrame(columns=[\"review\", \"rating\"], data=get_reviews(\"test\", \"neg\", neg_test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_pos, train_neg], ignore_index=True)\n",
    "test_df = pd.concat([test_pos, test_neg], ignore_index=True)"
   ]
  },
  {
   "source": [
    "## 1.1 Regression Model\n",
    "Use a feedforward neural network and NLP techniques we've seen up to now to train the best model you can on this dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"english\")\n",
    "pca = PCA(n_components=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.sample(n=1000, random_state=42)\n",
    "df = df.reset_index(drop=True)\n",
    "df.rating = df.rating.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review = df.review.apply(lambda t: \" \".join([t for t in t.replace(\"<br />\", \"\").lower().split(\" \") if not t in sw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = text.TfidfVectorizer()\n",
    "X = tf.fit_transform(df['review'])\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rev_tfidf\"] = [x for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review  rating  \\\n",
       "0  panic streets richard widmark plays u.s. navy ...     8.0   \n",
       "1  ask first one really better one. look sarah m....     1.0   \n",
       "2  big fan faerie tale theatre i've seen one best...    10.0   \n",
       "3  finished reading book dillinger. movie horribl...     1.0   \n",
       "4  greg davis bryan daly take crazed statements t...     2.0   \n",
       "\n",
       "                                           rev_tfidf  \n",
       "0  [-0.06693332477562777, 0.026501666720696388, 0...  \n",
       "1  [0.054588756233047195, 0.007289225635799655, 0...  \n",
       "2  [0.04245640578172975, -0.055399678198844185, 0...  \n",
       "3  [0.03061162246109369, -0.045177533976975846, -...  \n",
       "4  [-0.07803153617788054, 0.0048380035415081105, ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n      <th>rev_tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>panic streets richard widmark plays u.s. navy ...</td>\n      <td>8.0</td>\n      <td>[-0.06693332477562777, 0.026501666720696388, 0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ask first one really better one. look sarah m....</td>\n      <td>1.0</td>\n      <td>[0.054588756233047195, 0.007289225635799655, 0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>big fan faerie tale theatre i've seen one best...</td>\n      <td>10.0</td>\n      <td>[0.04245640578172975, -0.055399678198844185, 0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>finished reading book dillinger. movie horribl...</td>\n      <td>1.0</td>\n      <td>[0.03061162246109369, -0.045177533976975846, -...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>greg davis bryan daly take crazed statements t...</td>\n      <td>2.0</td>\n      <td>[-0.07803153617788054, 0.0048380035415081105, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_stopper = EarlyStopping(monitor=\"loss\", patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=X.shape[-1]))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/25\n",
      "1000/1000 [==============================] - 4s 2ms/step - loss: 20.6124 - accuracy: 0.1633\n",
      "Epoch 2/25\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 5.2688 - accuracy: 0.1855\n",
      "Epoch 3/25\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 4.5452 - accuracy: 0.1621\n",
      "Epoch 4/25\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 4.1323 - accuracy: 0.1600\n",
      "Epoch 5/25\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 4.0514 - accuracy: 0.1616\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "model.fit(x=X, y=df.rating, batch_size=1, epochs=25, callbacks=[loss_stopper]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test_df.sample(n=1000, random_state=42)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test.rating = df_test.rating.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = text.TfidfVectorizer()\n",
    "Xt = tf.fit_transform(df_test['review'])\n",
    "Xt = Xt.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = pca.fit_transform(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    preds[i] = round(preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.085"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "accuracy_score(preds, df_test.rating.values)"
   ]
  },
  {
   "source": [
    "## 1.2 RNN model\n",
    "Train a RNN to do the sentiment analysis regression. The RNN should consist simply of an embedding layer (to make word IDs into word vectors) a recurrent blocks (GRU or LSTM) feeding into an output layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(token):\n",
    "    \n",
    "    tags = []\n",
    "    \n",
    "    for tag in nltk.pos_tag(token):\n",
    "        tags.append(tag[1])\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.sample(n=1000, random_state=42)\n",
    "df = df.reset_index(drop=True)\n",
    "df.rating = df.rating.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rev_token\"] = df[\"review\"].apply(lambda x: nltk.word_tokenize(x))\n",
    "# df[\"rev_tag\"] = df[\"rev_token\"].apply(lambda x: get_tag(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review  rating  \\\n",
       "0  In Panic In The Streets Richard Widmark plays ...     8.0   \n",
       "1  If you ask me the first one was really better ...     1.0   \n",
       "2  I am a big fan a Faerie Tale Theatre and I've ...    10.0   \n",
       "3  I just finished reading a book about Dillinger...     1.0   \n",
       "4  Greg Davis and Bryan Daly take some crazed sta...     2.0   \n",
       "\n",
       "                                           rev_token  \n",
       "0  [In, Panic, In, The, Streets, Richard, Widmark...  \n",
       "1  [If, you, ask, me, the, first, one, was, reall...  \n",
       "2  [I, am, a, big, fan, a, Faerie, Tale, Theatre,...  \n",
       "3  [I, just, finished, reading, a, book, about, D...  \n",
       "4  [Greg, Davis, and, Bryan, Daly, take, some, cr...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n      <th>rev_token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In Panic In The Streets Richard Widmark plays ...</td>\n      <td>8.0</td>\n      <td>[In, Panic, In, The, Streets, Richard, Widmark...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>If you ask me the first one was really better ...</td>\n      <td>1.0</td>\n      <td>[If, you, ask, me, the, first, one, was, reall...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I am a big fan a Faerie Tale Theatre and I've ...</td>\n      <td>10.0</td>\n      <td>[I, am, a, big, fan, a, Faerie, Tale, Theatre,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I just finished reading a book about Dillinger...</td>\n      <td>1.0</td>\n      <td>[I, just, finished, reading, a, book, about, D...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Greg Davis and Bryan Daly take some crazed sta...</td>\n      <td>2.0</td>\n      <td>[Greg, Davis, and, Bryan, Daly, take, some, cr...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 \n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    return lexicon\n",
    "\n",
    "rev_lexicon = make_lexicon(df['rev_token'])\n",
    "# tag_lexicon = make_lexicon(df['rev_tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon_lookup(lexicon):\n",
    "\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    return lexicon_lookup\n",
    "\n",
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq] for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "df['Sentence_Idxs'] = tokens_to_idxs(df['rev_token'], rev_lexicon)\n",
    "# df['Tag_Idxs'] = tokens_to_idxs(df['rev_tag'], tag_lexicon)\n",
    "\n",
    "# tags_lexicon_lookup = get_lexicon_lookup(tag_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in df['Sentence_Idxs']])\n",
    "\n",
    "train_padded_words = pad_idx_seqs(df['Sentence_Idxs'], max_seq_len + 1)\n",
    "# train_padded_tags = pad_idx_seqs(df['Tag_Idxs'], max_seq_len + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_input_len, n_input_nodes, n_embedding_nodes, n_hidden_nodes, stateful=False, batch_size=20):\n",
    "    \n",
    "    input_layer = Input(shape=(None,))\n",
    "    \n",
    "    #Layer 2\n",
    "    embedding_layer = Embedding(input_dim=n_input_nodes,\n",
    "                                output_dim=n_embedding_nodes,\n",
    "                                mask_zero=True)(input_layer) \n",
    "    \n",
    "    # Layer 3\n",
    "    gru_layer = GRU(units=n_hidden_nodes)(embedding_layer)\n",
    "\n",
    "    #Layer 4\n",
    "    output_layer = Dense(units=1)(gru_layer)\n",
    "\n",
    "    model = Model(inputs=[input_layer], outputs=output_layer)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(seq_input_len=train_padded_words.shape[-1] - 1,\n",
    "                     n_input_nodes=len(rev_lexicon) + 1,\n",
    "                     n_embedding_nodes=300,\n",
    "                     n_hidden_nodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 104s 2s/step - loss: 38.5406\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 97s 2s/step - loss: 10.5636\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 99s 2s/step - loss: 5.7270\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 100s 2s/step - loss: 2.0687\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 99s 2s/step - loss: 1.2029\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2040ba094f0>"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "model.fit(x=train_padded_words[:,1:], y=df.rating, batch_size=20, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([test_pos, test_neg], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.sample(n=1000, random_state=42)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "test_df[\"rev_token\"] = test_df[\"review\"].apply(lambda x: nltk.word_tokenize(x))\n",
    "# test_df[\"rev_tag\"] = test_df[\"rev_token\"].apply(lambda x: get_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rev_lexicon = make_lexicon(test_df['rev_token'])\n",
    "# test_tag_lexicon = make_lexicon(test_df['rev_tag'])\n",
    "\n",
    "# test_tags_lexicon_lookup = get_lexicon_lookup(test_tag_lexicon)\n",
    "\n",
    "test_df['Sentence_Idxs'] = tokens_to_idxs(test_df['rev_token'], test_rev_lexicon)\n",
    "# test_df['Tag_Idxs'] = tokens_to_idxs(test_df['rev_tag'], test_tag_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max([len(idx_seq) for idx_seq in test_df['Sentence_Idxs']])\n",
    "\n",
    "test_padded_words = pad_idx_seqs(test_df['Sentence_Idxs'], max_seq_len + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_padded_words[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.flatten()\n",
    "for i in range(len(preds)):\n",
    "    preds[i] = round(preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "accuracy_score(preds, test_df.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-chosen",
   "metadata": {},
   "source": [
    "# 2. (evil) XOR Problem\n",
    "\n",
    "Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequenceâ€™s end. Test the two approaches below:"
   ]
  },
  {
   "source": [
    "### 2.1 \n",
    "\n",
    "Generate a dataset of random <=100,000 binary strings of equal length <= 50. Train the LSTM; what is the maximum length you can train up to with precisison?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50\n",
    "COUNT = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_pair = lambda x: [x, not(x)]\n",
    "training = np.array([[bin_pair(random.choice([0, 1])) for _ in range(SEQ_LEN)] for _ in range(COUNT)])\n",
    "target = np.array([[bin_pair(x) for x in np.cumsum(example[:,0]) % 2] for example in training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(SEQ_LEN, 2), dtype='float32'))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 21s 8ms/step - loss: 0.6931 - accuracy: 0.5052\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6920 - accuracy: 0.5114\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5443 - accuracy: 0.7871\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.2359 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.1620 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.1203 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0920 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0717 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0563 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0446 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.fit(training, target, epochs=10, batch_size=128);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "randomly selected sequence: [0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1\n 1 0 1 0 0 0 1 1 1 0 1 1 1]\nprediction: 0\nconfidence: 99.22%\nactual: 0\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(training)\n",
    "i = random.randint(0, COUNT)\n",
    "chance = predictions[i,-1,0]\n",
    "print('randomly selected sequence:', training[i,:,0])\n",
    "print('prediction:', int(chance > 0.5))\n",
    "print('confidence: {:0.2f}%'.format((chance if chance > 0.5 else 1 - chance) * 100))\n",
    "print('actual:', np.sum(training[i,:,0]) % 2)"
   ]
  },
  {
   "source": [
    "### 2.2\n",
    "\n",
    "Generate a dataset of random <=200,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50\n",
    "COUNT = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_pair = lambda x: [x, not(x)]\n",
    "training = np.array([[bin_pair(random.choice([0, 1])) for _ in range(SEQ_LEN)] for _ in range(COUNT)])\n",
    "target = np.array([[bin_pair(x) for x in np.cumsum(example[:,0]) % 2] for example in training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(SEQ_LEN, 2), dtype='float32'))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 0.6931 - accuracy: 0.5094\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6645 - accuracy: 0.5557\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1839 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0943 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0564 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0351 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0060 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.fit(training, target, epochs=10, batch_size=128);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "randomly selected sequence: [0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0\n 1 1 0 1 0 0 0 0 1 1 1 0 0]\nprediction: 0\nconfidence: 100.00%\nactual: 0\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(training)\n",
    "i = random.randint(0, COUNT)\n",
    "chance = predictions[i,-1,0]\n",
    "print('randomly selected sequence:', training[i,:,0])\n",
    "print('prediction:', int(chance > 0.5))\n",
    "print('confidence: {:0.2f}%'.format((chance if chance > 0.5 else 1 - chance) * 100))\n",
    "print('actual:', np.sum(training[i,:,0]) % 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}