{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are a general framework for modeling sequence data and are particularly useful for natural language processing tasks, and reinforcement learning. \n",
    "\n",
    "At a high level, RNN encode sequences via a set of parameters (weights) that are optimized to predict some output variable. This notebook demonstrates the code needed to assemble an RNN model using the Keras library, as well as some data processing tools that facilitate building the model. \n",
    "\n",
    "![](images/rnn2.png)\n",
    "\n",
    "Here an RNN will be used to encode a sentence and assign a POS tag to each word. The model shown here is applicable to any dataset with a one-to-one mapping between the inputs and outputs. This involves any task where for each sequential unit (here, a word), there is some output unit (here, a POS tag) that should be assigned to that input unit.\n",
    "\n",
    "### Brown Corpus\n",
    "\n",
    "The [Brown Corpus](http://www.hit.uib.no/icame/brown/bcm.html) (download through NLTK [here](http://www.nltk.org/nltk_data/)) is a popular NLP resource that consists of 500 texts from a variety of sources, including news reports, academic essays, and fiction. Every word in the texts has been annotated with a POS tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Sentence</th>\n",
       "      <th>Tagged_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, fulton, county, grand, jury, said, friday, an, investigation, of, atlanta's, recent, primary, election, produced, ``, no, evidence, '', that, any, irregularitie...</td>\n",
       "      <td>[DET, NOUN, NOUN, ADJ, NOUN, VERB, NOUN, DET, NOUN, ADP, NOUN, ADJ, NOUN, NOUN, VERB, ., DET, NOUN, ., ADP, DET, NOUN, VERB, NOUN, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[the, jury, further, said, in, term-end, presentments, that, the, city, executive, committee, ,, which, had, over-all, charge, of, the, election, ,, ``, deserves, the...</td>\n",
       "      <td>[DET, NOUN, ADV, VERB, ADP, NOUN, NOUN, ADP, DET, NOUN, ADJ, NOUN, ., DET, VERB, ADJ, NOUN, ADP, DET, NOUN, ., ., VERB, DET, NOUN, CONJ, NOUN, ADP, DET, NOUN, ADP, NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, september-october, term, jury, had, been, charged, by, fulton, superior, court, judge, durwood, pye, to, investigate, reports, of, possible, ``, irregularities,...</td>\n",
       "      <td>[DET, NOUN, NOUN, NOUN, VERB, VERB, VERB, ADP, NOUN, ADJ, NOUN, NOUN, NOUN, NOUN, PRT, VERB, NOUN, ADP, ADJ, ., NOUN, ., ADP, DET, ADJ, NOUN, DET, VERB, VERB, ADP, NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[``, only, a, relative, handful, of, such, reports, was, received, '', ,, the, jury, said, ,, ``, considering, the, widespread, interest, in, the, election, ,, the, n...</td>\n",
       "      <td>[., ADV, DET, ADJ, NOUN, ADP, ADJ, NOUN, VERB, VERB, ., ., DET, NOUN, VERB, ., ., ADP, DET, ADJ, NOUN, ADP, DET, NOUN, ., DET, NOUN, ADP, NOUN, CONJ, DET, NOUN, ADP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, jury, said, it, did, find, that, many, of, georgia's, registration, and, election, laws, ``, are, outmoded, or, inadequate, and, often, ambiguous, '', .]</td>\n",
       "      <td>[DET, NOUN, VERB, PRON, VERB, VERB, ADP, ADJ, ADP, NOUN, NOUN, CONJ, NOUN, NOUN, ., VERB, ADJ, CONJ, ADJ, CONJ, ADV, ADJ, ., .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[it, recommended, that, fulton, legislators, act, ``, to, have, these, laws, studied, and, revised, to, the, end, of, modernizing, and, improving, them, '', .]</td>\n",
       "      <td>[PRON, VERB, ADP, NOUN, NOUN, VERB, ., PRT, VERB, DET, NOUN, VERB, CONJ, VERB, ADP, DET, NOUN, ADP, VERB, CONJ, VERB, PRON, ., .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[the, grand, jury, commented, on, a, number, of, other, topics, ,, among, them, the, atlanta, and, fulton, county, purchasing, departments, which, it, said, ``, are, ...</td>\n",
       "      <td>[DET, ADJ, NOUN, VERB, ADP, DET, NOUN, ADP, ADJ, NOUN, ., ADP, PRON, DET, NOUN, CONJ, NOUN, NOUN, VERB, NOUN, DET, PRON, VERB, ., VERB, ADV, VERB, CONJ, VERB, ADV, VE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[merger, proposed]</td>\n",
       "      <td>[NOUN, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[however, ,, the, jury, said, it, believes, ``, these, two, offices, should, be, combined, to, achieve, greater, efficiency, and, reduce, the, cost, of, administratio...</td>\n",
       "      <td>[ADV, ., DET, NOUN, VERB, PRON, VERB, ., DET, NUM, NOUN, VERB, VERB, VERB, PRT, VERB, ADJ, NOUN, CONJ, VERB, DET, NOUN, ADP, NOUN, ., .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[the, city, purchasing, department, ,, the, jury, said, ,, ``, is, lacking, in, experienced, clerical, personnel, as, a, result, of, city, personnel, policies, '', .]</td>\n",
       "      <td>[DET, NOUN, VERB, NOUN, ., DET, NOUN, VERB, ., ., VERB, VERB, ADP, VERB, ADJ, NOUN, ADP, DET, NOUN, ADP, NOUN, NOUN, NOUN, ., .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          Tokenized_Sentence  \\\n",
       "0  [the, fulton, county, grand, jury, said, friday, an, investigation, of, atlanta's, recent, primary, election, produced, ``, no, evidence, '', that, any, irregularitie...   \n",
       "1  [the, jury, further, said, in, term-end, presentments, that, the, city, executive, committee, ,, which, had, over-all, charge, of, the, election, ,, ``, deserves, the...   \n",
       "2  [the, september-october, term, jury, had, been, charged, by, fulton, superior, court, judge, durwood, pye, to, investigate, reports, of, possible, ``, irregularities,...   \n",
       "3  [``, only, a, relative, handful, of, such, reports, was, received, '', ,, the, jury, said, ,, ``, considering, the, widespread, interest, in, the, election, ,, the, n...   \n",
       "4            [the, jury, said, it, did, find, that, many, of, georgia's, registration, and, election, laws, ``, are, outmoded, or, inadequate, and, often, ambiguous, '', .]   \n",
       "5            [it, recommended, that, fulton, legislators, act, ``, to, have, these, laws, studied, and, revised, to, the, end, of, modernizing, and, improving, them, '', .]   \n",
       "6  [the, grand, jury, commented, on, a, number, of, other, topics, ,, among, them, the, atlanta, and, fulton, county, purchasing, departments, which, it, said, ``, are, ...   \n",
       "7                                                                                                                                                         [merger, proposed]   \n",
       "8  [however, ,, the, jury, said, it, believes, ``, these, two, offices, should, be, combined, to, achieve, greater, efficiency, and, reduce, the, cost, of, administratio...   \n",
       "9     [the, city, purchasing, department, ,, the, jury, said, ,, ``, is, lacking, in, experienced, clerical, personnel, as, a, result, of, city, personnel, policies, '', .]   \n",
       "\n",
       "                                                                                                                                                             Tagged_Sentence  \n",
       "0                                      [DET, NOUN, NOUN, ADJ, NOUN, VERB, NOUN, DET, NOUN, ADP, NOUN, ADJ, NOUN, NOUN, VERB, ., DET, NOUN, ., ADP, DET, NOUN, VERB, NOUN, .]  \n",
       "1  [DET, NOUN, ADV, VERB, ADP, NOUN, NOUN, ADP, DET, NOUN, ADJ, NOUN, ., DET, VERB, ADJ, NOUN, ADP, DET, NOUN, ., ., VERB, DET, NOUN, CONJ, NOUN, ADP, DET, NOUN, ADP, NO...  \n",
       "2  [DET, NOUN, NOUN, NOUN, VERB, VERB, VERB, ADP, NOUN, ADJ, NOUN, NOUN, NOUN, NOUN, PRT, VERB, NOUN, ADP, ADJ, ., NOUN, ., ADP, DET, ADJ, NOUN, DET, VERB, VERB, ADP, NO...  \n",
       "3  [., ADV, DET, ADJ, NOUN, ADP, ADJ, NOUN, VERB, VERB, ., ., DET, NOUN, VERB, ., ., ADP, DET, ADJ, NOUN, ADP, DET, NOUN, ., DET, NOUN, ADP, NOUN, CONJ, DET, NOUN, ADP, ...  \n",
       "4                                            [DET, NOUN, VERB, PRON, VERB, VERB, ADP, ADJ, ADP, NOUN, NOUN, CONJ, NOUN, NOUN, ., VERB, ADJ, CONJ, ADJ, CONJ, ADV, ADJ, ., .]  \n",
       "5                                          [PRON, VERB, ADP, NOUN, NOUN, VERB, ., PRT, VERB, DET, NOUN, VERB, CONJ, VERB, ADP, DET, NOUN, ADP, VERB, CONJ, VERB, PRON, ., .]  \n",
       "6  [DET, ADJ, NOUN, VERB, ADP, DET, NOUN, ADP, ADJ, NOUN, ., ADP, PRON, DET, NOUN, CONJ, NOUN, NOUN, VERB, NOUN, DET, PRON, VERB, ., VERB, ADV, VERB, CONJ, VERB, ADV, VE...  \n",
       "7                                                                                                                                                               [NOUN, VERB]  \n",
       "8                                   [ADV, ., DET, NOUN, VERB, PRON, VERB, ., DET, NUM, NOUN, VERB, VERB, VERB, PRT, VERB, ADJ, NOUN, CONJ, VERB, DET, NOUN, ADP, NOUN, ., .]  \n",
       "9                                           [DET, NOUN, VERB, NOUN, ., DET, NOUN, VERB, ., ., VERB, VERB, ADP, VERB, ADJ, NOUN, ADP, DET, NOUN, ADP, NOUN, NOUN, NOUN, ., .]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.set_option('display.max_colwidth', 170) #widen pandas rows display\n",
    "\n",
    "train_sents = pandas.read_csv('dataset/example_train_brown_corpus.csv', encoding='utf-8')\n",
    "\n",
    "#Get the word tokens and tags into a readable list format\n",
    "train_sents['Tokenized_Sentence'] = train_sents['Tokenized_Sentence'].apply(lambda sent: sent.lower().split(\"\\t\"))\n",
    "train_sents['Tagged_Sentence'] = train_sents['Tagged_Sentence'].apply(lambda sent: sent.split(\"\\t\"))\n",
    "\n",
    "train_sents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "The sentences have already been tokenized into words, so both the words in each sentence and the corresponding tags are represented as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      "LEXICON SAMPLE (812 total items):\n",
      "{'the': 2, 'fulton': 3, 'county': 4, 'grand': 5, 'jury': 6, 'said': 7, 'friday': 8, 'an': 9, 'investigation': 10, 'of': 11, \"atlanta's\": 12, 'recent': 13, 'primary': 14, 'election': 15, 'produced': 16, '``': 17, 'no': 18, 'evidence': 19, \"''\": 20, 'that': 21}\n",
      "TAGS:\n",
      "LEXICON SAMPLE (12 total items):\n",
      "{'DET': 2, 'NOUN': 3, 'ADJ': 4, 'VERB': 5, 'ADP': 6, '.': 7, 'ADV': 8, 'CONJ': 9, 'PRT': 10, 'PRON': 11, 'NUM': 12, '<UNK>': 1}\n"
     ]
    }
   ],
   "source": [
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    '''Create a lexicon for the words in the sentences as well as the tags'''\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
    "    print(dict(list(lexicon.items())[:20]))\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "print(\"WORDS:\")\n",
    "words_lexicon = make_lexicon(train_sents['Tokenized_Sentence'])\n",
    "print(\"TAGS:\")\n",
    "tags_lexicon = make_lexicon(train_sents['Tagged_Sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the model will output tags as indices, we'll obviously need to map each tag number back to its corresponding string representation in order to later interpret the output. We'll reverse the tags lexicon to create a lookup table to get each tag from its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON LOOKUP SAMPLE:\n",
      "{2: 'DET', 3: 'NOUN', 4: 'ADJ', 5: 'VERB', 6: 'ADP', 7: '.', 8: 'ADV', 9: 'CONJ', 10: 'PRT', 11: 'PRON', 12: 'NUM', 1: '<UNK>'}\n"
     ]
    }
   ],
   "source": [
    "'''Make a dictionary where the string representation of a lexicon item can be retrieved from its numerical index'''\n",
    "\n",
    "def get_lexicon_lookup(lexicon):\n",
    "    '''Make a dictionary where the string representation \n",
    "        of a lexicon item can be retrieved \n",
    "        from its numerical index\n",
    "    '''\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    print(\"LEXICON LOOKUP SAMPLE:\")\n",
    "    print(dict(list(lexicon_lookup.items())[:20]))\n",
    "    return lexicon_lookup\n",
    "\n",
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_sents['Sentence_Idxs'] = tokens_to_idxs(train_sents['Tokenized_Sentence'], words_lexicon)\n",
    "train_sents['Tag_Idxs'] = tokens_to_idxs(train_sents['Tagged_Sentence'], tags_lexicon)\n",
    "train_sents[['Tokenized_Sentence', 'Sentence_Idxs', 'Tagged_Sentence', 'Tag_Idxs']][:10]\n",
    "\n",
    "tags_lexicon_lookup = get_lexicon_lookup(tags_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Numerical lists to matrices</font>\n",
    "\n",
    "Finally, we need to put the input sequences into matrices for training. \n",
    "\n",
    "here will be separate matrices for the word and tag sequences, where each row is a sentence and each column is a word (or tag) index in that sentence. \n",
    "\n",
    "This matrix format is necessary for the model to process the sentences in batches as opposed to one at a time, which significantly speeds up training. \n",
    "\n",
    "However, each sentence has a different number of words, so we create a padded matrix equal to the length on the longest sentence in the training set. For all sentences with fewer words, we prepend the row with zeros representing an empty word (and tag) position. This is why the number 0 was not assigned as an index in the lexicons. \n",
    "\n",
    "We can specify to Keras to ignore these zeros during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      " [[  0   0   0 ...  24  25  26]\n",
      " [  0   0   0 ...  46  47  26]\n",
      " [  0   0   0 ...  66  67  26]\n",
      " ...\n",
      " [  0   0   0 ... 758  20  26]\n",
      " [  0   0   0 ... 802  34 447]\n",
      " [  0   0   0 ... 447 812  26]]\n",
      "SHAPE: (100, 60) \n",
      "\n",
      "TAGS:\n",
      " [[0 0 0 ... 5 3 7]\n",
      " [0 0 0 ... 5 5 7]\n",
      " [0 0 0 ... 3 3 7]\n",
      " ...\n",
      " [0 0 0 ... 3 7 7]\n",
      " [0 0 0 ... 3 7 3]\n",
      " [0 0 0 ... 3 3 7]]\n",
      "SHAPE: (100, 60) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in train_sents['Sentence_Idxs']]) # Get length of longest sequence\n",
    "train_padded_words = pad_idx_seqs(train_sents['Sentence_Idxs'], \n",
    "                                  max_seq_len + 1) #Add one to max length for offsetting sequence by 1\n",
    "train_padded_tags = pad_idx_seqs(train_sents['Tag_Idxs'],\n",
    "                                 max_seq_len + 1)  #Add one to max length for offsetting sequence by 1\n",
    "\n",
    "print(\"WORDS:\\n\", train_padded_words)\n",
    "print(\"SHAPE:\", train_padded_words.shape, \"\\n\")\n",
    "\n",
    "print(\"TAGS:\\n\", train_padded_tags)\n",
    "print(\"SHAPE:\", train_padded_tags.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Defining the input and output</font>\n",
    "\n",
    "In this approach, for each word in a sentence, we predict the tag for that word based on two types of input: \n",
    "\n",
    "1. all the words in the sentence up to that point, including that current word\n",
    "\n",
    "2. all the previous tags in the sentence. \n",
    "\n",
    "So for a given position in the sentence *idx*, the input is `train_padded_words[idx]` and `train_padded_tags[idx-1]`, and the output is `train_padded_tags[idx]`. The example below shows this alignment for the first sentence in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Word</th>\n",
       "      <th>Input Tag</th>\n",
       "      <th>Output Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>-</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fulton</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>county</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grand</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jury</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>said</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>friday</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>an</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>investigation</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>of</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>atlanta's</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>recent</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>primary</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>election</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>produced</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>``</td>\n",
       "      <td>VERB</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>no</td>\n",
       "      <td>.</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>evidence</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>''</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>that</td>\n",
       "      <td>.</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>any</td>\n",
       "      <td>ADP</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>irregularities</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>took</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>place</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>.</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Input Word Input Tag Output Tag\n",
       "0              the         -        DET\n",
       "1           fulton       DET       NOUN\n",
       "2           county      NOUN       NOUN\n",
       "3            grand      NOUN        ADJ\n",
       "4             jury       ADJ       NOUN\n",
       "5             said      NOUN       VERB\n",
       "6           friday      VERB       NOUN\n",
       "7               an      NOUN        DET\n",
       "8    investigation       DET       NOUN\n",
       "9               of      NOUN        ADP\n",
       "10       atlanta's       ADP       NOUN\n",
       "11          recent      NOUN        ADJ\n",
       "12         primary       ADJ       NOUN\n",
       "13        election      NOUN       NOUN\n",
       "14        produced      NOUN       VERB\n",
       "15              ``      VERB          .\n",
       "16              no         .        DET\n",
       "17        evidence       DET       NOUN\n",
       "18              ''      NOUN          .\n",
       "19            that         .        ADP\n",
       "20             any       ADP        DET\n",
       "21  irregularities       DET       NOUN\n",
       "22            took      NOUN       VERB\n",
       "23           place      VERB       NOUN\n",
       "24               .      NOUN          ."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "pandas.DataFrame(list(zip(train_sents['Tokenized_Sentence'].loc[0],\n",
    "                          [\"-\"] + train_sents['Tagged_Sentence'].loc[0],\n",
    "                          train_sents['Tagged_Sentence'].loc[0])),\n",
    "                 columns=['Input Word', 'Input Tag', 'Output Tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This same alignment is shown below for a sentence in the padded matrices. Because of the offsetting in the alignment, the length of the padded matrices will be reduced by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Input Words  Input Tags  Output Tags\n",
      "0             0           0            0\n",
      "1             0           0            0\n",
      "2             0           0            0\n",
      "3             0           0            0\n",
      "4             0           0            0\n",
      "5             0           0            0\n",
      "6             0           0            0\n",
      "7             0           0            0\n",
      "8             0           0            0\n",
      "9             0           0            0\n",
      "10            0           0            0\n",
      "11            0           0            0\n",
      "12            0           0            0\n",
      "13            0           0            0\n",
      "14            0           0            0\n",
      "15            0           0            0\n",
      "16            0           0            0\n",
      "17            0           0            0\n",
      "18            0           0            0\n",
      "19            0           0            0\n",
      "20            0           0            0\n",
      "21            0           0            0\n",
      "22            0           0            0\n",
      "23            0           0            0\n",
      "24            0           0            0\n",
      "25            0           0            0\n",
      "26            0           0            0\n",
      "27            0           0            0\n",
      "28            0           0            0\n",
      "29            0           0            0\n",
      "30            0           0            0\n",
      "31            0           0            0\n",
      "32            0           0            0\n",
      "33            0           0            0\n",
      "34            2           0            2\n",
      "35            3           2            3\n",
      "36            4           3            3\n",
      "37            5           3            4\n",
      "38            6           4            3\n",
      "39            7           3            5\n",
      "40            8           5            3\n",
      "41            9           3            2\n",
      "42           10           2            3\n",
      "43           11           3            6\n",
      "44           12           6            3\n",
      "45           13           3            4\n",
      "46           14           4            3\n",
      "47           15           3            3\n",
      "48           16           3            5\n",
      "49           17           5            7\n",
      "50           18           7            2\n",
      "51           19           2            3\n",
      "52           20           3            7\n",
      "53           21           7            6\n",
      "54           22           6            2\n",
      "55           23           2            3\n",
      "56           24           3            5\n",
      "57           25           5            3\n",
      "58           26           3            7\n"
     ]
    }
   ],
   "source": [
    "print(pandas.DataFrame(list(zip(train_padded_words[0,1:], train_padded_tags[0,:-1], train_padded_tags[0, 1:])),\n",
    "                columns=['Input Words', 'Input Tags', 'Output Tags']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Building the model</font>\n",
    "\n",
    "### <font color='#6629b2'>Functional API</font>\n",
    "\n",
    "To set up the model, we'll use Keras [Functional API](https://keras.io/getting-started/functional-api-guide/), which is one of two ways to assemble models in Keras (the alternative is the [Sequential API](https://keras.io/getting-started/sequential-model-guide/), which is a bit simpler but has more constraints). For the POS tagger model, new tags will be predicted from the combination of two input sequences, the words in the sentence and the corresponding tags in the sentence. The Functional API is specifically useful when a model has multiple inputs and/or outputs. A model consists of a series of layers. As shown in the code below, we initialize instances for each layer. Each layer can be called with another layer as input, e.g. Embedding()(input_layer). A model instance is initialized with the Model() object, which defines the initial input and final output layers for that model. Before the model can be trained, the compile() function must be called with the loss function and optimization algorithm specified (see below).\n",
    "\n",
    "### <font color='#6629b2'>Layers</font>\n",
    "\n",
    "We'll build an RNN with the following layers, numbered according to the level on which they are stacked:\n",
    "\n",
    "**1. Input (words)**: This input layer takes in a sequence of word indices.\n",
    "\n",
    "**1. Input (tags)**: This is the other input layer alongside the first, and it takes in a sequence of tag indices. It is on the same level as the word input layer, so both input sequences are read in parallel by the model.\n",
    "\n",
    "**2. Embedding (words)**: There are two embedding layers, one for the words and a different one for the tags. Both of them function the same way: they convert the indices into distributed vector representations (embeddings). The mask_zero=True parameter indicates that values of 0 in the matrix (the padding) will be ignored by the model.\n",
    "\n",
    "**2. Embedding (tags)**: Same as the word embedding layer, but for the tags.\n",
    "\n",
    "**3. Concatenate**: This layer merges each embedded word sequence and corresponding embedded tag sequence into a single sequence. This means that for a given word and the tag for that word, their vectors will be concatenated into a single vector.\n",
    "\n",
    "**4. GRU**: The recurrent (GRU) hidden layer reads the merged embedded sequence and computes a representation (hidden state) of the sequence. The result is a new vector for each word/tag in the sequence. There are a few architectures for this layer - I use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer. By specifying return_sequences=True in the below function, this layer will output the entire sequence of vectors (hidden states) for the sequence, rather than just the most recent hidden state that is returned by default.\n",
    "\n",
    "![](images/gru.png)\n",
    "\n",
    "**5. (Time Distributed) Dense**: An output layer that produces a probability distribution for each possible tag for each word in the sequence. The 'softmax' activation is what transforms the values of this layer into scores from 0 to 1 that can be treated as probabilities. The Dense layer produces the probability scores for one particular timepoint (word). By wrapping this in a TimeDistributed() layer, the model outputs a probability distribution for every timepoint in the sequence. \n",
    "\n",
    "The term \"layer\" is just an abstraction, when really all these layers are just matrices. Each layer is connected to the layer above it via a set of weights (also matrices), which are the parameters that are adjusted during training in order for the model to learn to predict tags. The process of training a neural network is a series of matrix multiplications. \n",
    "\n",
    "### <font color='#6629b2'>Parameters</font>\n",
    "\n",
    "Our function for creating the model takes the following parameters:\n",
    "\n",
    "**seq_input_length**: the length of the padded matrices for the word and tag sentence inputs, which will be the same since there is a one-to-one mapping between tags. This is equal to the length of the longest sentence in the training data. \n",
    "\n",
    "**n_word_input_nodes**: the number of unique words in the lexicon, plus one to account for matrix padding represented by 0 values. This indicates the number of rows in the word embedding layer, where each row corresponds to a word.\n",
    "\n",
    "**n_tag_input_nodes**: the number of unique tags in the dataset, plus one to account for padding. This indicates the number of rows in the tag embedding layer, where each row corresponds to a tag.\n",
    "\n",
    "**n_word_embedding_nodes**: the number of dimensions in the word embedding layer, which can be freely defined. Here, it is set to 300.\n",
    "\n",
    "**n_tag_embedding_nodes**: the number of dimensions in the tag embedding layer, which can be freely defined. Here, it is set to 100.\n",
    "\n",
    "**n_hidden_nodes**: the number of dimensions in the hidden layer. Like the embedding layers, this can be freely chosen. Here, it is set to 500.\n",
    "\n",
    "**stateful**: By default, the GRU hidden layer will reset its state (i.e. its values will be 0s) each time a new set of sequences is read into the model.  However, when stateful=True is given, this parameter indicates that the GRU hidden layer should \"remember\" its state until it is explicitly told to forget it. In other words, the values in this layer will be carried over between separate calls to the training function. This is useful when processing long sequences, so that the model can iterate through chunks of the sequences rather than loading the entire matrix at the same time, which is memory-intensive. I'll show below how this setting is also useful when tagging new sequences. Here, because the training sequences only consist of one sentence, stateful will be set to False during training. At prediction time, it will be set to True.\n",
    "\n",
    "**batch_size**: It is not always necessary to specify the batch size when setting up a Keras model. The fit() function will apply batch processing by default and the batch size can be given as a parameter. However, when a model is stateful, the batch size does need to be specified in the Input() layers. Here, for training, batch_size=None, so Keras will use its default batch size (which is 32). During prediction, the batch size will be set to 1.\n",
    "\n",
    "### <font color='#6629b2'>Procedure</font>\n",
    "\n",
    "The output of the model is a sequence of vectors, each with the same number of dimensions as the number of unique tags (n_tag_input_nodes). Each vector contains the predicted probability of each possible tag for the corresponding word in that position in the sequence. Like all neural networks, RNNs learn by updating the parameters (weights) to optimize an objective (loss) function applied to the output. For this model, the objective is to minimize the cross entropy (named as the \"sparse_categorical_crossentropy\" in the code) between the predicted tag probabilities and the probabilities observed from the tags in training data, resulting in probabilities that more accurately predict when a particular tag will appear. This is the general procedure used for all multi-label classification tasks. Updates to the weights of the model are performed using an optimization algorithm, such as Adam used here. The details of this process are extensive; see the resources at the bottom of the notebook if you want a deeper understanding. One huge benefit of Keras is that it implements many of these details for you. Not only does it already have implementations of the types of layer architectures, it also has many of the [loss functions](https://keras.io/losses/) and [optimization methods](https://keras.io/optimizers/) you need for training various models. The specific loss function and optimization method you use is specified when compiling the model with the model.compile() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create the model'''\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, TimeDistributed, Dense\n",
    "from tensorflow.keras.layers import Embedding, GRU\n",
    "\n",
    "def create_model(seq_input_len, n_word_input_nodes, n_tag_input_nodes, n_word_embedding_nodes,\n",
    "                 n_tag_embedding_nodes, n_hidden_nodes, stateful=False, batch_size=None):\n",
    "    \n",
    "    #Layers 1\n",
    "    word_input = Input(batch_shape=(batch_size, seq_input_len), name='word_input_layer')\n",
    "    tag_input = Input(batch_shape=(batch_size, seq_input_len), name='tag_input_layer')\n",
    "\n",
    "    #Layers 2\n",
    "    word_embeddings = Embedding(input_dim=n_word_input_nodes,\n",
    "                                output_dim=n_word_embedding_nodes, \n",
    "                                mask_zero=True, name='word_embedding_layer')(word_input) #mask_zero will ignore 0 padding\n",
    "    #Output shape = (batch_size, seq_input_len, n_word_embedding_nodes)\n",
    "    tag_embeddings = Embedding(input_dim=n_tag_input_nodes,\n",
    "                               output_dim=n_tag_embedding_nodes,\n",
    "                               mask_zero=True, name='tag_embedding_layer')(tag_input) \n",
    "    #Output shape = (batch_size, seq_input_len, n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 3\n",
    "    merged_embeddings = Concatenate(axis=-1, name='concat_embedding_layer')([word_embeddings, tag_embeddings])\n",
    "    #Output shape =  (batch_size, seq_input_len, n_word_embedding_nodes + n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 4\n",
    "    hidden_layer = GRU(units=n_hidden_nodes, return_sequences=True, \n",
    "                       stateful=stateful, name='hidden_layer')(merged_embeddings)\n",
    "    #Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "    \n",
    "    #Layer 5\n",
    "    output_layer = TimeDistributed(Dense(units=n_tag_input_nodes, \n",
    "                                         activation='softmax'), name='output_layer')(hidden_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model = Model(inputs=[word_input, tag_input], outputs=output_layer)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
    "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_word_embedding_nodes=300,\n",
    "                     n_tag_embedding_nodes=100,\n",
    "                     n_hidden_nodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Training</font>\n",
    "\n",
    "Now we're ready to train the model. We'll call the fit() function to train the model for 10 iterations through the dataset (epochs), using a batch size of 20 sentences. Keras reports to cross-entropy loss after each epoch, which should continue to decrease if the model is learning correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.5010\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.3959\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.3039\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 0.2220\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.1578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13e57bf10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Train the model'''\n",
    "\n",
    "# output matrix (y) has extra 3rd dimension added because sparse cross-entropy function requires one label per row\n",
    "model.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
    "          y=train_padded_tags[:, 1:, None], \n",
    "          batch_size=20, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Tagging new sentences</font>\n",
    "\n",
    "Now that the model is trained, it can be used to predict tags in new sentences in the test set. As opposed to training where we processed multiple sentences at the same time, it will be more straightforward to demonstrate tagging on a single sentence at a time. In Keras, you can duplicate a model by loading the parameters from a saved model into a new model. Here, this new model will have a batch size of 1. It will also process a sentence one word/tag at a time (seq_input_len=1) and predict the next tag, using the stateful=True parameter to remember its previous predictions within that sentence. The other parameters of this prediction model are exactly the same as the trained model, which is why the weights can be readily transferred. To demonstrate prediction performance, I'll load the weights from a saved model previously trained on the full training set of 51606 sentences (as opposed to 100 sentences in the example dataset used above). I'll apply the model to an example test set of 100 sentences that were not observed during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON LOOKUP SAMPLE:\n",
      "{2: 'DET', 3: 'NOUN', 4: 'ADJ', 5: 'VERB', 6: 'ADP', 7: '.', 8: 'ADV', 9: 'CONJ', 10: 'PRT', 11: 'PRON', 12: 'NUM', 1: '<UNK>'}\n"
     ]
    }
   ],
   "source": [
    "'''Create predictor model with weights from saved model, with batch_size = 1, seq_input_len = 1 and stateful=True'''\n",
    "tags_lexicon_lookup = get_lexicon_lookup(tags_lexicon)\n",
    "\n",
    "predictor_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load the test set and apply same processing steps performed above for training set'''\n",
    "\n",
    "test_sents = pandas.read_csv('dataset/example_test_brown_corpus.csv', encoding='utf-8')\n",
    "test_sents['Tokenized_Sentence'] = test_sents['Tokenized_Sentence'].apply(lambda sent: sent.lower().split(\"\\t\"))\n",
    "test_sents['Tagged_Sentence'] = test_sents['Tagged_Sentence'].apply(lambda sent: sent.split(\"\\t\"))\n",
    "test_sents['Sentence_Idxs'] = tokens_to_idxs(test_sents['Tokenized_Sentence'], words_lexicon)\n",
    "test_sents['Tag_Idxs'] = tokens_to_idxs(test_sents['Tagged_Sentence'], tags_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll iterate through the sentences in the test set and tag each of them. For each sentence, we start with an empty list for the predicted tags. For the first word in the sentence, there is no previous tag, so the model reads that word and the empty tag 0 (the padding value). The predict() function returns a probability distribution over the tags, and we pick the tag with the highest probability as the one to assign that word. This tag is appended to our list of predicted tags, and we continue to the next word in the sentence. Because the model is stateful, we can simply provide the current word and most recent tag as input to the predict() function, since its hidden layer has memorized the sequence of words/tags observed so far. After the entire sentence has been tagged, we call reset_states() to clear the values for this sentence so we can process a new sentence. The tag indices are mapped back to their string forms, which I show in the sample below, alongside the correct (gold) tags for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 59) for input KerasTensor(type_spec=TensorSpec(shape=(None, 59), dtype=tf.float32, name='word_input_layer'), name='word_input_layer', description=\"created by layer 'word_input_layer'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 59) for input KerasTensor(type_spec=TensorSpec(shape=(None, 59), dtype=tf.float32, name='tag_input_layer'), name='tag_input_layer', description=\"created by layer 'tag_input_layer'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "SENTENCE:\the\twas\tabout\t50\tyears\told\t.\n",
      "PREDICTED:\tVERB\tVERB\tADP\tNOUN\tNOUN\tVERB\t.\n",
      "GOLD:\t\tPRON\tVERB\tADV\tNUM\tNOUN\tADJ\t. \n",
      "\n",
      "\n",
      "SENTENCE:\t``\tanother\tyoung\tman\t,\tmy\tdear\t?\t?\n",
      "PREDICTED:\tVERB\tDET\tNOUN\tNOUN\t.\tDET\tNOUN\tNOUN\tNOUN\n",
      "GOLD:\t\t.\tDET\tADJ\tNOUN\t.\tDET\tNOUN\t.\t. \n",
      "\n",
      "\n",
      "SENTENCE:\treally\t,\tyou\tare\tmost\tindiscreet\tto\tdrive\thim\there\tyourself\t''\t,\the\tsaid\t,\tfrowning\twith\tdispleasure\t.\n",
      "PREDICTED:\tVERB\t.\tDET\tVERB\tADV\tVERB\tPRT\tVERB\tPRON\tVERB\tDET\t.\t.\tPRON\tVERB\t.\tDET\tADP\tNOUN\t.\n",
      "GOLD:\t\tADV\t.\tPRON\tVERB\tADV\tADJ\tPRT\tVERB\tPRON\tADV\tPRON\t.\t.\tPRON\tVERB\t.\tVERB\tADP\tNOUN\t. \n",
      "\n",
      "\n",
      "SENTENCE:\tdelphine\tpresented\ther\tcheek\tfor\ta\tkiss\t,\tand\tthe\tphysician\tpecked\tit\tlike\ta\ttimid\trooster\t.\n",
      "PREDICTED:\tVERB\tDET\tNOUN\tNOUN\tADP\tDET\tNOUN\t.\tCONJ\tDET\tNOUN\tNOUN\tPRON\tVERB\tDET\tNOUN\tNOUN\t.\n",
      "GOLD:\t\tNOUN\tVERB\tDET\tNOUN\tADP\tDET\tNOUN\t.\tCONJ\tDET\tNOUN\tVERB\tPRON\tADP\tDET\tADJ\tNOUN\t. \n",
      "\n",
      "\n",
      "SENTENCE:\t``\tdandy\tis\tto\tbe\tour\thouse\tguest\t,\tlouis\t.\n",
      "PREDICTED:\tVERB\tDET\tVERB\tPRT\tVERB\tDET\tNOUN\tNOUN\t.\tDET\t.\n",
      "GOLD:\t\t.\tNOUN\tVERB\tPRT\tVERB\tDET\tNOUN\tNOUN\t.\tNOUN\t. \n",
      "\n",
      "\n",
      "SENTENCE:\ti\twant\tthe\troom\tin\tthe\tattic\tprepared\tfor\thim\the\tis\ta\tmost\tunusual\tlad\t,\tquite\tprecocious\tin\tmany\tways\t.\n",
      "PREDICTED:\tVERB\tDET\tDET\tNOUN\tADP\tDET\tNOUN\tNOUN\tADP\tPRON\tVERB\tVERB\tDET\tADJ\tNOUN\tNOUN\t.\tDET\tNOUN\tADP\tNOUN\tNOUN\t.\n",
      "GOLD:\t\tPRON\tVERB\tDET\tNOUN\tADP\tDET\tNOUN\tVERB\tADP\tPRON\tPRON\tVERB\tDET\tADV\tADJ\tNOUN\t.\tADV\tADJ\tADP\tADJ\tNOUN\t. \n",
      "\n",
      "\n",
      "SENTENCE:\the\tdeserves\ta\tbetter\tlife\tthan\tjust\trotting\taway\ton\tthe\tprieur\tplantation\t''\t.\n",
      "PREDICTED:\tVERB\tVERB\tDET\tNOUN\tNOUN\tADP\tADP\tNOUN\tNOUN\tADP\tDET\tNOUN\tNOUN\t.\t.\n",
      "GOLD:\t\tPRON\tVERB\tDET\tADJ\tNOUN\tADP\tADV\tVERB\tADV\tADP\tDET\tNOUN\tNOUN\t.\t. \n",
      "\n",
      "\n",
      "SENTENCE:\t``\tquite\tso\t,\tmy\tdear\t.\n",
      "PREDICTED:\tVERB\tDET\tADP\t.\tDET\tNOUN\t.\n",
      "GOLD:\t\t.\tADV\tADV\t.\tDET\tNOUN\t. \n",
      "\n",
      "\n",
      "SENTENCE:\this\troom\twill\tbe\tready\tshortly\t''\t.\n",
      "PREDICTED:\tVERB\tDET\tVERB\tVERB\tADJ\t.\t.\t.\n",
      "GOLD:\t\tDET\tNOUN\tVERB\tVERB\tADJ\tADV\t.\t. \n",
      "\n",
      "\n",
      "SENTENCE:\tthe\tphysician\tled\tthe\thorses\tto\tthe\tstable\tafter\ta\tcursory\tglance\tat\tthe\tcringing\tslave\t.\n",
      "PREDICTED:\tVERB\tDET\tNOUN\tDET\tNOUN\tPRT\tDET\tNOUN\tADP\tDET\tNOUN\tNOUN\tADP\tDET\tNOUN\tNOUN\t.\n",
      "GOLD:\t\tDET\tNOUN\tVERB\tDET\tNOUN\tADP\tDET\tNOUN\tADP\tDET\tADJ\tNOUN\tADP\tDET\tVERB\tNOUN\t. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Predict tags for sentences in test set'''\n",
    "\n",
    "import numpy\n",
    "\n",
    "pred_tags = []\n",
    "for _, sent in test_sents.iterrows():\n",
    "    tok_sent = sent['Tokenized_Sentence']\n",
    "    sent_idxs = sent['Sentence_Idxs']\n",
    "    sent_gold_tags = sent['Tagged_Sentence']\n",
    "    sent_pred_tags = []\n",
    "    prev_tag = 0  #initialize predicted tag sequence with padding\n",
    "    for cur_word in sent_idxs:\n",
    "        # cur_word and prev_tag are just integers, but the model expects an input array\n",
    "        # with the shape (batch_size, seq_input_len), so prepend two dimensions to these values\n",
    "        p_next_tag = predictor_model.predict(x=[numpy.array(cur_word)[None, None],\n",
    "                                                numpy.array(prev_tag)[None, None]])[0]\n",
    "        prev_tag = numpy.argmax(p_next_tag, axis=-1)[0]\n",
    "        sent_pred_tags.append(prev_tag)\n",
    "    predictor_model.reset_states()\n",
    "\n",
    "    #Map tags back to string labels\n",
    "    sent_pred_tags = [tags_lexicon_lookup[tag] for tag in sent_pred_tags]\n",
    "    pred_tags.append(sent_pred_tags) #filter padding \n",
    "\n",
    "test_sents['Predicted_Tagged_Sentence'] = pred_tags\n",
    "\n",
    "#print sample\n",
    "for _, sent in test_sents[:10].iterrows():\n",
    "    print(\"SENTENCE:\\t{}\".format(\"\\t\".join(sent['Tokenized_Sentence'])))\n",
    "    print(\"PREDICTED:\\t{}\".format(\"\\t\".join(sent['Predicted_Tagged_Sentence'])))\n",
    "    print(\"GOLD:\\t\\t{}\".format(\"\\t\".join(sent['Tagged_Sentence'])), \"\\n\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Evaluation</font>\n",
    "\n",
    "We can evaluate our model with some of the standard metrics for classification: *precision*, *recall*, and *F1 score*. In the context of this task, precision is the proportion of the predicted tags for a particular class that were correct predictions (i.e. of all the words that were assigned a NOUN tag by the tagger, what percentage of these were actually nouns according to the test set?). Recall is the proportion of correct tags for a particular class that the tagger also predicted correctly (i.e. of all the words in the test set that should have been assigned a NOUN tag, what percentage of these were actually tagged as a NOUN?). F1 score is a weighted average of precision and recall. The scikit-learn package has several of these [evaluation metrics](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.657\n",
      "PRECISION: 0.685\n",
      "RECALL: 0.657\n",
      "F1: 0.621\n"
     ]
    }
   ],
   "source": [
    "'''Evalute the model by precision, recall, and F1'''\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_gold_tags = [tag for sent_tags in test_sents['Tagged_Sentence'] for tag in sent_tags]\n",
    "    all_pred_tags = [tag for sent_tags in test_sents['Predicted_Tagged_Sentence'] for tag in sent_tags]\n",
    "    accuracy = accuracy_score(y_true=all_gold_tags, y_pred=all_pred_tags)\n",
    "    precision = precision_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "    recall = recall_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "    f1 = f1_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "\n",
    "    print(\"ACCURACY: {:.3f}\".format(accuracy))\n",
    "    print(\"PRECISION: {:.3f}\".format(precision))\n",
    "    print(\"RECALL: {:.3f}\".format(recall))\n",
    "    print(\"F1: {:.3f}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Conclusion</font>\n",
    "\n",
    "Even though this model can accuractely predict many POS tags, state-of-the-art taggers use more sophisticated techniques. For example, where here we predicted a tag just based on the preceding words and tags, [bidirectional layers](https://keras.io/layers/wrappers/#bidirectional) also model the sequence that appears after the given word to additionally inform the prediction. POS tagging can be seen as a shallow version of syntactic parsing, which is a more difficult NLP problem. Where POS tagging outputs a flat sequence with a one-to-one mapping between words and tags, syntatic parsing produces a hierarchical structure where categories consist of multiple-word phrases and phrase categories are embedded inside other phrases. Check out the [chapter from Jurafsky & Martin's book](https://web.stanford.edu/~jurafsky/slp3/14.pdf) if you're interested in learning more about these deeper models of linguistic structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>More resources</font>\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) \n",
    "\n",
    "[Chris Olah on how LSTM RNNs work](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "Denny Britz's [tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}